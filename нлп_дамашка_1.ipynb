{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ec76d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fake_useragent in c:\\users\\swl\\anaconda3\\lib\\site-packages (0.1.11)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import requests\n",
    "!{sys.executable} -m pip install fake_useragent\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff77436",
   "metadata": {},
   "source": [
    "# Step 1. Collecting data\n",
    "\n",
    "Я буду использовать пользовательсие отзывы на анимацию на метакритике."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba50c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "570cf9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting individual film pages\n",
    "def page(url):\n",
    "    u_a = UserAgent(verify_ssl=False)\n",
    "    response = session.get(url, headers={'User-Agent': u_a.random})\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = soup.find_all('a', {'class': 'metascore_anchor'})\n",
    "    links = list(map(lambda x: 'https://www.metacritic.com' + x.attrs['href'], links))\n",
    "    return links[::3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f84596cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = ['https://www.metacritic.com/browse/movies/genre/userscore/animation?view=detailed', 'https://www.metacritic.com/browse/movies/genre/userscore/animation?view=detailed&page=5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0a5db2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = list(chain(*list(map(page, links))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "33e25456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting reviews and corresponding scores\n",
    "def film(url):\n",
    "    u_a = UserAgent(verify_ssl=False)\n",
    "    response = session.get(url, headers={'User-Agent': u_a.random})\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    reviews = soup.find_all('span', {'class': 'blurb blurb_expanded'})\n",
    "    cl = re.compile(\"metascore_w user(.+)\")\n",
    "    marks = soup.find_all('div', {'class':cl})\n",
    "    try:\n",
    "        reviews = list(map(lambda x: re.sub(r'<br/>', ' ', str(x)), reviews))\n",
    "        reviews = list(map(lambda x: re.search(r'ed\">(.+?)</span>', x).group(1), reviews))\n",
    "        marks = list(map(lambda x: float(x.text), marks))\n",
    "    except:\n",
    "        return ('', 0)\n",
    "    return list(zip(marks, reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0c33f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattening out\n",
    "pairs = list(chain(*list(map(film, pages))))\n",
    "clean_pairs = list(filter(lambda x: type(x)==tuple, pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "319eae95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing down\n",
    "with open('reviews.csv', 'w', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, delimiter=' ')\n",
    "    for rr in clean_pairs:\n",
    "        writer.writerow(list(rr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76c40d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shortcut reading out of  file in case of restart\n",
    "with open('reviews.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=' ')\n",
    "    clean_pairs = list(filter(lambda x: len(x) == 2, [*reader]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4733f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_pairs = sorted(clean_pairs, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d82627a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.0',\n",
       " \"The universal acclaim about this movie is to be understood by the atmosphere it creates. Everywhere your look goes you see bright colors and movements and animations you simply can't understand. Because 'Promare' definitively is aggressive, moving and clocked. Characters are attractive, intelligently connected an funny. You can find differents sub-stories that evolve in parallel with the main plot and those add some content to the characters it concerns without dragging us out of the movie. Far from being perfect, visual effects do bring a lot in 'Promare' and there is something charming when you watch it. However, story is way too conventional and there is no turn of events unless those who are fundamental to japan-animation movies. I enjoyed to discover the universe in which Promare people live but I was less intrigued by the resolution that you can predict from miles and miles away. Animation is colorful yes but jerky and often really hard to follow. Shapes and colors don't stop hitting your retina so fast you wish you could simply zoom out this mess. The particularity of the animation is that it swings between amazing and cartoon-esque. Some vehicules are gorgeous to watch while faces in overall seem to have been drawn by a 8-year-old kid.  The main criticism I have to address to 'Promare' is that I felt we don't show us enough of the past stories: there are a lot of flashbacks and complex connections between characters that are not correctly explained by the movie itself. I suppose I don't have to underline the fact the central protagonist is a stereotype of himslelf: an intelligent, sociable and strong boy/man with a dumb motto and unlikely hairstyle.\"]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fin_pairs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f258a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating\n",
    "good_revs = list(map(lambda x: x[1], fin_pairs[-210:]))\n",
    "bad_revs = list(map(lambda x: x[1], fin_pairs[:210]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d61027f",
   "metadata": {},
   "source": [
    "# Step 2. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d04a2210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e82bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(pp):\n",
    "    tok = word_tokenize(pp.lower())\n",
    "    depunct = list(filter(lambda x: x not in punctuation, tok))\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    lems = list(map(lambda x: lemmer.lemmatize(x), depunct))\n",
    "    return lems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c3ce0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_lems = list(map(clean_up, good_revs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b69e2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_lems = list(map(clean_up, bad_revs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a969924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating testing samples\n",
    "g_lems_ch = g_lems[:10]\n",
    "g_lems = g_lems[10:]\n",
    "b_lems_ch = b_lems[:10]\n",
    "b_lems = b_lems[10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32738fa1",
   "metadata": {},
   "source": [
    "# Step 3. Word-lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cafa89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b83773ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_lems = list(chain(*g_lems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9356f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_lems = list(chain(*b_lems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7fcddd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gco = Counter(g_lems)\n",
    "bco = Counter(b_lems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03b1604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_in(l1,l2):  #elements of l1 that are not in l2\n",
    "    return list(filter(lambda x: x not in l2.keys(), list(l1.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53c30ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_g = not_in(gco, bco)\n",
    "uniq_b = not_in(bco, gco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c0c7116",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only in good + more frequent than 2 times\n",
    "freq_uniq_g = list(filter(lambda x: gco[x]>2, uniq_g))\n",
    "#only in bad + more frequent than 2 times\n",
    "freq_uniq_b = list(filter(lambda x: bco[x]>2, uniq_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "250e251b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many words were cut off by frequency?\n",
    "len(freq_uniq_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76aecad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2778"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uniq_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6481d1ed",
   "metadata": {},
   "source": [
    "# Step 4. +/- function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a4fef00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detector(l_words, g_list,  b_list):  # gets a preprocessed list of words of a comment+ freqlists\n",
    "    goods = list(filter(lambda x: x in g_list, l_words))\n",
    "    bads = list(filter(lambda x: x in b_list, l_words))\n",
    "    if len(goods) < len(bads):\n",
    "            return -1\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "14592366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector(g_lems_ch[5], uniq_g, uniq_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a261aade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\swl\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\swl\\anaconda3\\lib\\site-packages (from sklearn) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\swl\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.20.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\swl\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\swl\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\swl\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4415a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "537d4755",
   "metadata": {},
   "outputs": [],
   "source": [
    "answ = [1]*10 + [-1]*10\n",
    "datas = g_lems_ch + b_lems_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a12c8e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no frequency threshold\n",
    "predict = list(map(lambda x: detector(x, uniq_g, uniq_b), datas))\n",
    "accuracy_score(predict, answ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "964d4ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  frequencies >2\n",
    "predict = list(map(lambda x: detector(x, freq_uniq_g, freq_uniq_b), datas))\n",
    "accuracy_score(predict, answ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559758cf",
   "metadata": {},
   "source": [
    "# Step 5. Приблуды"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f038717",
   "metadata": {},
   "source": [
    "1) Хочется попробовать использовать только оценочные слова - с наибольшей вероятностью это прилагательные.\n",
    "Попробую выделить только их через pos-тэги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c4d942a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6d1ea9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Swl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c35fbfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poss(wrds):\n",
    "    return [x[0] for x in pos_tag(wrds) if x[1]=='JJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e3afb05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great', 'kinda', 'great', 'i', 'i', 'black', 'memorable', 'great', 'cool']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poss(datas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "964eb4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no frequency threshold + adj-filter\n",
    "pos_datas = list(map(poss, datas))\n",
    "predict = list(map(lambda x: detector(x, uniq_g, uniq_b), pos_datas))\n",
    "accuracy_score(predict, answ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6a6ccead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  frequencies >2 + adj-filter\n",
    "predict = list(map(lambda x: detector(x, freq_uniq_g, freq_uniq_b), pos_datas))\n",
    "accuracy_score(predict, answ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05880dd8",
   "metadata": {},
   "source": [
    "Стало хуже.\n",
    "2) Можно попробовать почистить датасет. Во-первых, там есть несколько отзывов на итальянском - можно убрать их пакетным определителем языка каким-то. Во-вторых, судя по всему, некоторые пользоватеи забывали ставить оценку в системе и ставили её только текстом в отзыве - из-за этого есть некоторое количество отзывов с оценкой 0.0, но с положительным текстом, в котором эксплицитно написано 8/10. Можно попробовать искать цифры в отзывах и отсеивать такие выбросы."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
